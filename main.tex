\documentclass[letterpaper]{article}
\usepackage[letterpaper]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{algpseudocode} 
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{mathtools}

\title{Towards Better Coordinate Descent Using Bandits}
\date{September 2019}

\usepackage{natbib}
\usepackage{graphicx}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\istar}{i^*}
\newcommand{\comment}[1]{%
  \text{\phantom{(#1)}} \tag{#1}
}

\begin{document}

\maketitle


\section{Introduction and related work}

Coordinate descent is a powerful method for optimizing objective functions, especially those with undesirable properties (e.g., nonsmooth regularizers). Because computing a single update to the objective can be done in constant time, coordinate descent methods also hold promise in optimizing high-dimensional problems, where dimension dependence in the iteration complexity is undesirable. However, the method for choosing which coordinate to update strongly affects the convergence rate of CD; an active area of research is how to choose coordinates more intelligently to achieve provable convergence improvements \emph{while} not sacrificing coordinate descent's cheap update rule.

The goal of this project is to improve upon the work of~\cite{salehi2018coordinate}, who introduce a bandit-based coordinate descent algorithm achieving a convergence rate of $t = O(d^2/\epsilon)$. While the dependence on suboptimality is polynomially better than prior work, the dependence on dimension is quadratic in the worst case (nevertheless, the authors argue that in practice, their algorithm is fast and demonstrate this using empirical experiments; this raises the question of whether better bounds can be obtained for their approach). One curious aspect of the analysis of~\cite{salehi2018coordinate} is that they do not use a regret-type bound to to bound their bandit algorithm in terms of their full-information algorithm; rather, they bound their bandit algorithm directly\footnote{They do this while making explicit assumptions about how well their algorithm chooses arms relative to OPT. In particular, they assume that their algorithm chooses an arm with reward at most $c$ times worse than the optimal arm, and they claim that can control this constant by varying the epoch length and exploration parameter $\varepsilon$. However, they provide no analysis into these dependencies, so in this respect their bounds for their bandit algorithm are weak}.

Another work that uses bandits for coordinate descent is~\cite{namkoong2017adaptive}. Unlike~\cite{salehi2018coordinate}, these authors \emph{make use of explicit regret bounds} to get bounds on the performance of their bandit algorithm in terms of the full-information adversary. They use rewards that are proportional to $|\nabla_i f(x)|^2$. These authors give bounds under the assumption of power-law distributed distributed rewards\footnote{In their work, this means that $|\nabla_i f(x)| \leq \beta i^{-\alpha}$ for some $\alpha \in [1, \infty)$.}. In this setting, the authors report the following iteration complexity:
\begin{equation}
    \left(\frac{R}{\epsilon}\right)^2 d^{2-\alpha} + \left(\frac{R}{\epsilon}\right)^\frac{4}{3} d \log^\frac{5}{3} d
\end{equation}
in the case where $\alpha \in (1,2)$, and
\begin{equation}
    \left(\frac{R}{\epsilon}\right) \log^2 d + \left(\frac{R}{\epsilon}\right)^\frac{4}{3} d \log^\frac{5}{3} d
\end{equation}
in the case where $\alpha \geq 2$. Unlike~\cite{salehi2018coordinate}, this work does use a regret-style bound in their analysis and hence \emph{does not} make explicit assumptions about the gap between the bandit algorithm's choices and the optimal (which~\cite{salehi2018coordinate} does through the constant $c(E, \epsilon)$). Although these bounds have worse dependence on $\epsilon$, they are stronger in that they do not make explicit assumptions about the performance of the bandit's choices.

Some questions:
\begin{itemize}
    \item As mentioned above, the performance of the bandit algorithm~\cite{salehi2018coordinate} is strongly influenced by the parameter $c$, which is a function of the epoch length $E$ and exploration probability $\varepsilon$. Curiously, the authors find that performance \emph{degrades} when using a $\varepsilon$ probability greater than 0, i.e., \emph{exploration only hurts}. Is there an explanation for this?
    \item ~\cite{salehi2018coordinate} provide limited computational experiments and do not provide their code. Perhaps we should replicate their empirical results. This might give more insight into the behavior of $c(E, \epsilon)$.
\end{itemize}


\section{Problem statement}

\textbf{Problem statement:} Let's assume that we have some good dynamic regret bound on our algorithm:

\begin{equation}
    \sum_i^T\text{Reward}(ALG_i) - \sum_i^T \text{Reward}(OPT_i) \leq \text{Reg}(V, T)
    \label{eq:regret}
\end{equation}

Here, $V$ is the \emph{path-length} of the comparator sequence. Typically, this is defined as
\begin{equation}
    V \leq \sum_{t=1}^T |\text{Reward}(OPT_t) - \text{Reward}(OPT_{t-1})|
\end{equation}

Depending on how we define our rewards (discussed later), this affect the type of bound we can get on $V$.

\section{Naive approach}
We present a naive approach for deriving convergence rates for a bandit-based coordinate descent algorithm \emph{where the suboptimality (or the objective) is used directly as a reward signal}. This is not practical, since computing the objective is requires $O(d)$ work, which is precisely what we are trying to avoid by using a bandit algorithm.

Let the reward be defined as the change in suboptimality ($\epsilon(\cdot)\equiv f(\cdot) - \inf_x f(x)$) from the beginning:

\begin{equation}
    \text{Reward}(ALG_i) \equiv R^t_i = \epsilon(x^0) - \epsilon(x^t)
    \label{eq:naive_reward}
\end{equation}
where $x^t$ are chosen by $ALG$ (to denote the iterates of the comparator $OPT$, we use $R^t_{\istar}$ and $x_*^t$).
Now the regret bound (Eq.~\ref{eq:regret}) becomes:
\begin{equation}
    \sum_{t=1}^T R_{i}^t - \sum_{t=1}^T R_{\istar}^t \leq -\text{Reg}(V,T)
\end{equation}
Using Eq.~\ref{eq:naive_reward}, we get
\begin{equation}
    \sum_{t=1}^T \epsilon(x^t) \leq \sum_{t=1}^T \epsilon(x^t_*) + \text{Reg}(V,T)
\end{equation}
We can bound the average suboptimality by dividing both sides by $T$:
\begin{equation}
    \frac{1}{T}\sum_{t=1}^T \epsilon(x^t) \leq \frac{1}{T} \sum_{t=1}^T \epsilon (x^T_*) + \frac{\text{Reg}(V,T)}{T}
\end{equation}
Now, using the guaranteed descent property (i.e., using the step sizes in the paper can only improve the objective), we know that the suboptimality of the final iterate is smaller than the average
\begin{equation}
    \epsilon(x^T) \leq \frac{1}{T}\sum_{t=1}^T \epsilon(x^t) \leq \frac{1}{T} \sum_{t=1}^T \epsilon(x_*^t) + \frac{\text{Reg}(V,T)}{T}
    \label{eq:naive_average_regret}
\end{equation}

Now, let's assume we have an optimal convergence rate of the form
\begin{equation}
    \epsilon(x^t_*) \leq \frac{C}{t}
\end{equation}
Then Eq.~\ref{eq:naive_average_regret} implies
\begin{equation}
    \epsilon(x^t) \leq \frac{2C\log t + \text{Reg}(V,T)}{t}
\end{equation}

What is the form of $\text{Reg}(V,T)$ in this scenario? If we use path-length bounds from~\cite{bubeck2019improved} (corresponding to Algorithm 1), we can get a regret bound of the form
\begin{equation}
    \text{Reg}(V, T, d) \leq O\left( \frac{d \log T}{\gamma} + \gamma V\right)
\end{equation}

for some controllable parameter $\gamma$ (satisfying $\gamma \leq 1/162$).

This leads to a sublinear convergence rate for the bandit-based algorithm
\begin{equation}
    \epsilon(x^t) \leq O\left(\frac{C \log t + d \log t / \gamma + \gamma V}{t}\right)
\end{equation}

In~\cite{salehi2018coordinate}, we have the following bound for the full-information algorithm:
\begin{equation}
    \epsilon(x_*^t) \leq \frac{8L^2 \eta^2 / \beta}{2d + t - t_0}
\end{equation}
where $\eta$ is an upper bound on the ratio of duality gap to the best coordinate-wise duality gap, i.e. $\min_i G(x^t_*)/G_i(x^t_*)$. In the worst case, $\eta = O(d)$, but the authors argue that a more common case is when $\eta = O(1)$~\cite{salehi2018coordinate}\footnote{\textbf{research question}: when is it the case that $\eta = O(1)$? This is the case when the rewards are power-law distributed, but this is not a standard assumption about objective functions. Can we relate this scenario to a more ``well-known" property of objective functions? (Formalizing this might help us know when using an adaptive or non-uniform sampling algorithm will help coordinate descent converge faster)}.

Thus, in the worst case, this leads our bandit algorithm to have a convergence rate of
\begin{equation}
    \epsilon(x^t) \leq \frac{8L^2 \eta^2}{2d+t-t_0} + \frac{d \log t + \gamma^2 V}{\gamma t}
    \label{eq:naive_convergence_rate}
\end{equation}

When $t \gg d$ \emph{and} we have good bounds on $V$ (see section~\ref{section:naive_bounding_path_length}), this gives the worst-case $t = \tilde{O}(d^2/\epsilon)$; this, however, is only the case when all coordinates have the same $G_i$'s. In the case where these rewards are power-law distributed (i.e., like the case analyzed in~\cite{namkoong2017adaptive}), we can guarantee that $\eta = O(\log d)$ in the worst case (i.e., the case where the rewards decay like $1/i$) and so we get a nice iteration complexity of $t = \tilde{O}(\log^2 d / \epsilon)$

\subsection{Bounding the path length in the naive setting}
\label{section:naive_bounding_path_length}
In the naive setting (Eq.~\ref{eq:naive_reward}), it is easy to bound the path length of $V$. 

\begin{align}
    V &= \sum_{t=1}^T |R_{\istar}^t - R_{\istar}^{t-1}| \\
    &= \sum_{t=1}^T |\epsilon(x^{t-1}_*)-\epsilon(x^t_*)| \\
    &= \epsilon(x^0_*) - \epsilon(x^T_*)\\
    &\leq \epsilon(x^0_*)
\end{align}
This does not strongly affect Eq.~\ref{eq:naive_convergence_rate}.

\textbf{Note}: If our rewards/losses decrease fast enough, we can get good path-length bounds (and therefore the final path-length term becomes negligible).

\section{Beyond the naive approach}
In the \emph{naive setting} (where we use the suboptimality---or equivalently the objective value---as a reward signal), we can derive meaningful convergence rates using regret bounds (Eq.~\ref{eq:naive_convergence_rate}). However, in order to obtain these regret bounds, we need to compute the reward from pulling a particular arm; this requires us to recompute the suboptimality/objective value on each iteration. Computing this requires $O(d)$ work, so this diminishes the utility of the naive approach.

\textbf{A question arises}: can we use the machinery introduced in~\cite{salehi2018coordinate}, coupled with these improved regret bounds from, e.g.,~\cite{bubeck2019improved}, to derive a better bandit-based coordinate descent algorithm? 

Some ideas:
\begin{enumerate}
    \item[(a)] Use the same rewards as~\cite{salehi2018coordinate}. This poses a problem is that these rewards are decreasing with $T$, so a regret bound like $O(\log T)$ does not allow for guaranteed convergence of our bandit algorithm.
    \item[(b)] Use the same rewards as~\cite{salehi2018coordinate} but rescale them with time; e.g., $\text{Reward} = t\cdot r_i^t$. At first glance, this solves some of the problems with the first case, but this does not work because it `disincentivizes' the bandit algorithm from descending early. In particular, the bandit can achieve a lower regret by choosing bad arms early and good arms later, rather than choosing good arms early and good arms later.
    \item[(c)] We can use the sum of the rewards in~\cite{salehi2018coordinate} \emph{as a proxy} for the true suboptimality, in an attempt to replicate the results for the naive case. This approach also has challenges; see section~\ref{section:proxy_rewards}
\end{enumerate}

\subsection{Reusing rewards from~\cite{salehi2018coordinate}}
\label{section:reusing_rewards}
To demonstrate why (a) doesn't work, let's try to use the rewards in~\cite{salehi2018coordinate} as our reward function (for which we can apply regret bounds). First note that the reward $r_i^t$ is an lower bound on the improvement in the objective:
\begin{align}
    r_i^t &\leq f(x^{t-1}) - f(x^t) \\
    &=\epsilon(x^{t-1}) - \epsilon(x^t)
\end{align}

Now we show how If we use these as our rewards, our regret bound becomes
\begin{equation}
    \sum_{t=1}^T r_i^t \leq \sum_{t=1}^T r_{\istar}^t - \text{Reg}(V,T)
\end{equation}
Divide through by $T$: 
\begin{equation}
    -\frac{1}{T} \sum_{t=1}^T r_i^t \leq -\frac{1}{T}\sum_{t=1}^T r_{\istar}^t + \frac{\text{Reg}(V,T)}{T}
\end{equation}
Now, substituting $\epsilon(x^t) - \epsilon(x^{t-1}) \leq -r_i^t$, we have
\begin{align}
    \frac{1}{T}\left[\epsilon(x^T) - \epsilon(x^0)\right]&= -\frac{1}{T}\sum_{t=1}^T\left(\epsilon(x^t) - \epsilon(x^{t-1})\right)\\
    &\leq -\frac{1}{T} \sum_{t=1}^T r_i^t \\
    &\leq -\frac{1}{T}\sum_{t=1}^T r_{\istar}^t + \frac{\text{Reg}(V,T)}{T}
\end{align}
Now, we can use some of the properties of $r_i^t$ as described in~\cite{salehi2018coordinate}. For sake of argument, let's assume we are in case (a) of the proof of Theorem 2, i.e. $s_{\istar}^t = 1$ (see p. 15-16 of~\cite{salehi2018coordinate}). In this situation, we have that
\begin{align}
    -r_{\istar}^t &\leq -\frac{G_{\istar}(x^t_*)}{2} \\
    &\leq -\frac{G(x^t_*)}{2d} \\
    &\leq -\frac{\epsilon(x^t_*)}{2d}
    \label{eq:reward_subgap}
\end{align}

Therefore,
\begin{align}
    \frac{1}{T}\left[\epsilon(x^T) - \epsilon(x^0)\right] &\leq -\frac{1}{T}\sum_{t=1}^T \epsilon(x^t_*) + \frac{\text{Reg}(V,T)}{T} \\
\end{align}
Now we hit a wall, because we defined our rewards according to (a). Note that this will (at best) give us a suboptimality like $\epsilon(x^T) \leq O\left(1 + \text{Reg}(V,T)\right)\equiv O(\log T)$, so our algorithm doesn't even converge. This shows the need to redefine our rewards.

\subsection{Redefining rewards}
\label{section:proxy_rewards}

Let's redefine our rewards as follows:
\begin{equation}
    R_i^T \equiv \sum_{t=1}^T r_i^t
\end{equation}
and
\begin{equation}
    R_{\istar}^T \equiv \sum_{t=1}^T r_{\istar}^t
\end{equation}
Note that this is a lower bound on the net improvement in objective, i.e., $\epsilon(x^t) - \epsilon(x^0) \leq -R_i^t$. Now our regret bound becomes
\begin{equation}
    \sum_{t=1}^T R_i^t -\sum_{t=1}^T R_{\istar}^t \geq -\text{Reg}(V,T)
\end{equation}
Rearranging terms and dividing by $T$, we get
\begin{align}
    \frac{1}{T}\sum_{t=1}^T \epsilon(x^t) - \epsilon(x^0) &\leq -\sum_{t=1}^T R_i^t \\
    &\leq -\sum_{t=1}^T R_{\istar}^t + \text{Reg}(V,T)\\
    &= -\sum_{t=1}^T \sum_{s=1}^t r_{\istar}^s + \text{Reg}(V,T)
\end{align}
Recalling that the bandit algorithm is guaranteed to improve the objective (i.e., $\epsilon(x^{t+1}) \leq \epsilon(x^t)$), we know that the suboptimality of the final iterate is less than the average (i.e. $\epsilon(x^T) \leq \frac{1}{T} \sum_{t=1}^T \epsilon(x^t)$), so we get that
\begin{equation}
    \epsilon(x^T) \leq \epsilon(x^0) -\sum_{t=1}^T \sum_{s=1}^t r_{\istar}^s + \text{Reg}(V,T)
\end{equation}
Now we use the relations from Eq.~\ref{eq:reward_subgap}:
\begin{align}
    \epsilon(x^T) &\leq \epsilon(x^0) - \frac{1}{T}\sum_{t=1}^T \sum_{s=1}^t \frac{\epsilon(x^t_*)}{2d} + \frac{\text{Reg(V,T)}}{T} \\
    % &\leq \epsilon(x^0) - \frac{1}{T}\sum_{t=1}^T \sum_{s=1}^t \frac{\epsilon(x^t_*)}{2d} + \frac{\text{Reg(V,T)}}{T}
\end{align}
Divide through both sides by $\epsilon(x^T)$

\begin{align}
    1&\leq \frac{\epsilon(x^0) + \frac{\text{Reg(V,T)}}{T}}{\epsilon(x^T)} - \frac{1}{T}\sum_{t=1}^T \sum_{s=1}^t \frac{\epsilon(x^t_*)}{2d\epsilon(x^T)} \\
\end{align}
Rearranging

\begin{align}
    \frac{\epsilon(x^0) + \frac{\text{Reg(V,T)}}{T}}{\epsilon(x^T)} &\geq 1 + \frac{1}{T}\sum_{t=1}^T \sum_{s=1}^t \frac{\epsilon(x^t_*)}{2d\epsilon(x^T)}\\
    &\geq 1 + \frac{1}{T}\sum_{t=1}^T \sum_{s=1}^t \frac{\epsilon(x^t_*)}{2d\epsilon(x^{t_0})}
    \label{eq:t0}
\end{align}
where we have assumed that $T > t_0$, and hence $\epsilon(x^{t_0}) \geq \epsilon(x^{T})$. Using Eq. (35) from~\cite{salehi2018coordinate}, i.e.,
\footnote{this equation only holds for max\_r, i.e., the full information greedy algorithm. To ensure that this holds for our bandit algorithm, we run $t_0 = \max(1, 2d \log(\frac{d \beta \epsilon(x^0)}{4L^2 \eta^2}))$ iterations of greedy before starting our bandit algorithm}
\begin{equation}
    \epsilon(x^{t_0}) \leq \frac{4L^2 \eta^2}{\beta d}
\end{equation}
So \ref{eq:t0} becomes

\begin{align}
    \frac{\epsilon(x^0) + \frac{\text{Reg(V,T)}}{T}}{\epsilon(x^T)} &\geq 1 + \frac{\beta}{8L^2 \eta^2 T}\sum_{t=1}^T \sum_{s=1}^t \epsilon(x^t_*)
    \label{eq:t0}
\end{align}

% \textbf{As of 10/5/19, this is where I'm stuck. I've thought about possibly using a lower bound on $\epsilon(x_*^t)$ here in order to cancel out the first term $\epsilon(x^0)$.}

\section{A new definition for rewards}
\label{sec:new_definition_for_rewards}
Teodor suggested a new way to define rewards that makes the analysis more amenable to using a bandit regret bound. 

The idea is to treat the ratio $\eta_t = G(x^t_*) ||a_i|| / G_i(x^t_*)$ as a reward that can vary based on which arm is pulled.

Let's show how this idea can be applied. For simplicity, let's consider the case where $s_i^t < 1$. Start with equation (32) in~\cite{salehi2018coordinate}:

\begin{align}
    r_{\istar}^t &\geq \frac{G_{\istar}^2 (x^t_*) \beta}{8L^2 ||a_i||^2} \\
    &= \frac{G(x^t) \beta}{8L^2 \eta_t^2}
\end{align}

Using the descent property of $r_i^t$, this yields:
\begin{align}
    \epsilon(x^{t+1}) - \epsilon(x^t) \leq -r_{\istar}^t \leq -\frac{\epsilon(x^t)\beta}{8L^2 \eta_t^2}
\end{align}

From this point on,~\cite{salehi2018coordinate} use induction to prove their bound. However, we appeal to the standard analysis for GD on smooth objectives:

\begin{align}
    \epsilon(x^{t+1}) &\leq \epsilon(x^t) \left(1 - \frac{\epsilon(x^t) \beta}{8L^2 \eta_t^2}\right) \\ 
    \frac{1}{\epsilon(x^t)} &\leq \frac{1}{\epsilon(x^{t+1})} - \frac{\beta}{8L^2 \eta_t^2} \comment{divide both sides by $\epsilon(x^t)\epsilon(x^{t+1}$)}
\end{align}

Telescoping the sum, we get:
\begin{align}
    \frac{1}{\epsilon(x^t)} \geq \frac{1}{\epsilon(x^0)} + \frac{\beta}{8L^2} \sum_{i=1}^t \frac{1}{\eta_i^2}
\end{align}

So
\begin{align}
    \epsilon(x^t_*) \leq \frac{1}{\frac{1}{\epsilon(x^0)}  + \frac{\beta}{8L^2} \sum_{i=1}^t \frac{1}{\eta_i^2}}
\end{align}

Notice that the quantity in the denominator $\sum_{i=1}^t 1/\eta_i^2$ is the sum of the rewards (for a suitable definition of reward $1/\eta_t^2$), and so a regret bound can be applied to it:

\begin{equation}
    \epsilon(x^t) \leq \frac{1}{\frac{1}{\epsilon(x^0)}  + \frac{\beta}{8L^2} \sum_{i=1}^t \frac{1}{\eta_i^2} - \text{Reg}(V,T)}
\end{equation}

Since the best sequence under this model of rewards is that which choses $i = \argmin \eta_{t,i}$, if we use the original assumption from the paper we have that $\eta_{t,i} \leq \eta$, implying that

\begin{equation}
    \epsilon(x^t) \leq \frac{1}{\frac{1}{\epsilon(x^0)}  + \frac{\beta T}{8L^2 \eta^2} - \text{Reg}(V,T)}
\end{equation}

\subsection{Computing these rewards}

One problem with this new definition for rewards is computational intractibility. Computing $\eta_t = G(x^t) ||a_i||/G_i(x^t) = \frac{||a_i||}{G_i(x^t)} \sum_{i=1}^d G_i(x^t)$ requires summing over $d$ coordinates, which is too much work. As discussed already, we would like to compute rewards in $O(1)$ time.

One way to deal with this is to \emph{amortize} the cost of computing $G$ over multiple iterations, and substitute $G(x^t)$ with some estimator of it. 
Suppose we compute $\hat{G}(x^t) = G(x^{E\floor{t/E}})$, where $E$ is a parameter called the \emph{epoch length}. If $E = \Omega(d)$, then computing $\hat{G}(x^t)$ requires only $O(1)$ amortized time.


What is the relationship between the estimator $\hat{G}(x^t)$ and the true duality gap $G(x^t)$? Since our algorithm always improves $G$, we know that $\hat{G}(x^t) \geq G(x^t)$. Moreover, let's assume that over an epoch length the duality gap does not decrease by more than a factor of $c$, i.e., $G(x^{t}) \leq c G(x^{t+E})$. 
So we have the following relationship:
\begin{equation}
    G(x^t) \leq \hat{G}(x^t) \leq c G(x^t)
\end{equation}
Now we can redefine our rewards yet again:
\begin{equation}
\hat{\eta}_t = \frac{\hat{G}(x^t) ||a_i||}{G_i(x^t)} \geq \eta_t
\end{equation}
This gives
\begin{align}
    \epsilon(x^t_*) &\leq \frac{1}{\frac{1}{\epsilon(x^0)}  + \frac{\beta}{8L^2} \sum_{i=1}^t \frac{1}{\eta_i^2}} \\
    &\leq \frac{1}{\frac{1}{\epsilon(x^0)}  + \frac{\beta}{8L^2} \sum_{i=1}^t \frac{1}{\hat{\eta}_i^2}}
    % &\leq \frac{1}{\frac{1}{\epsilon(x^0)}  + \frac{\beta}{8L^2} \sum_{i=1}^t \frac{1}{\hat{\eta}_i^2} - \text{Reg}(V,T)}
\end{align}
so the bandit suboptimality is 

\begin{align}
    \epsilon(x^t) &\leq\frac{1}{\frac{1}{\epsilon(x^0)}  + \frac{\beta}{8L^2} \left[\sum_{i=1}^t \frac{1}{\hat{\eta}_i^2} - \text{Reg}(V,T)\right]}
    % &\leq \frac{1}{\frac{1}{\epsilon(x^0)}  + \frac{\beta}{8L^2} \sum_{i=1}^t \frac{1}{\hat{\eta}_i^2} - \text{Reg}(V,T)}
\end{align}
where this regret is defined for the rounded rewards $1/\hat{\eta_t}^2$.


Since $\hat{\eta}_t \leq c\eta$, this leads to the bound
\begin{align}
    \epsilon(x^t) &\leq\frac{1}{\frac{1}{\epsilon(x^0)}  + \frac{\beta T}{8c^2 \eta^2 L^2} - \frac{\beta}{8 L^2} \text{Reg}(V,T)}    
    \label{eq:suboptimality_with_regret}
\end{align}

The constant $c$ can be viewed as analogous to the constant $c$ that appears in ~\cite{salehi2018coordinate}. However, here $c$ has a more clear interpretation: it bounds the change in objective over $E$ iterations.

\subsection{An example objective}
Let's consider a simple toy example to see how our bandit algorithm performs. Suppose we have a smooth objective function of the form
\begin{equation}
    F(x) = f(Ax) + \lambda_1 x_1^2 + \cdots + \lambda_d x_d^2
\end{equation}
where $\lambda_1 \gg \lambda_2 = \cdots = \lambda_d$. As shown in Lemma 2 of~\cite{salehi2018coordinate}, the update rule for this objective is
\begin{align*}
    \kappa_i &= -\frac{\nabla_i F(x)}{2\lambda_i} \\ 
    s_i &= \min\left\{1, \frac{3\lambda_i}{2\lambda_i + \frac{1}{\beta}}\right\} \\
    r_i &= \begin{cases}
        \frac{(\nabla_i F(x))^2}{8\lambda_i^2 \beta}(2\lambda_i \beta - 1) & \text{if }\lambda_i \geq 1/\beta \\
        \frac{3}{4}\frac{(\nabla_i F(x))^2}{2\lambda_i + \frac{1}{\beta}} & \text{otherwise} \\
    \end{cases}
\end{align*}
Assuming some random initialization and large enough $\lambda_1$, the best coordinate to pull is always $x_1$ in this situation. This is because the first condition in $r_i$ will be satisfied, and $r_i$ will have a linear dependence on $\lambda_i$. max\_r will discover this coordinate immediately, and our algorithm will also discover this coordiante on the first iteration (since it will compute the full $G = \sum_i G_i$).

So in this trivial example, our algorithm discovers the optimal coordinate.

\subsection{Another bound on the estimator}
Another way to understand the constant $c$ is in terms of the relationship between coordinates. Define the \emph{correspondence} between coordinates $i$ and $j$ in terms of the following bound:
\begin{equation}
    C_{ji}^t \equiv \frac{\frac{G_j^2(x^t) ||a_j||^2}{G^2(x^t)} - \frac{G_j^2(x^t + s_i^t \kappa_i^t e_i) ||a_j||^2}{G^2(x^t + s_i^t \kappa_i^t e_i)}}{\frac{G_j^2(x^t) ||a_i||^2}{G^2(x^t)}}
\end{equation}
i.e., the reward of pulling arm $j$ before pulling arm $i$ minus the reward of pulling arm $j$ after pulling arm $i$, normalized by the reward of pulling arm $j$.

Let's assume that we have an upper bound of the form
\begin{equation}
C_{ij}^t \leq \gamma \;\;\forall i, j \in [d], \forall t
\end{equation}
Let's assume that we update our estimate of the sum of the duality gaps for each coordinate, like in the bandit algorithm presented in~\cite{salehi2018coordinate}. Then we know that $c \leq (1-\gamma)^E$.

\subsection{Bounding the estimator directly}
The fact that the duality gap doesn't change too much is actually a direct consequence of the smoothness and Lipschitzness of the objective. Consider the definition of the coordinate-wise duality gap in the paper:
\begin{equation}
    G_i(x) = g_i^*(-a_i^T w) + g_i(x_i) + x_i a_i^T w
\end{equation}
Since each $g_i$ has $L_i$-bounded support, each $g_i^*$ is $L_i$-Lipschitz. Furthermore, we know that $f$ is $1/\beta$ smooth, implying that the gradient $\nabla f = w$ is $1/\beta$-Lipschitz. Let's assume our regularizer is $\lambda$-Lipschitz. So altogether the above function has a coordinate-wise Lipschitz constant of 
\begin{equation}
    \max\left(\frac{L_i||a_i||}{\beta},\lambda,\frac{||a_i||}{\beta}\right)\equiv c_i
\end{equation}
This implies that
\begin{align}
    G_i(x_i^t) - G_i(x_i^{t+1}) &\leq c |x_i^{t+1}-x_i^t| \\ 
    &= c_i s_i \kappa_i
\end{align}
Now we can bound the change total duality gap
\begin{equation}
    G(x^t) - G(x^{t+1}) \leq s_i \kappa_i \max_j c_j
    \label{eq:bounding_duality_gap}
\end{equation}
Suppose we are in the case where $s_i < 1$. Then we have that
\begin{equation}
s_i = \frac{G_i^t + \mu_i |\kappa_i|^2/2}{|\kappa_i^t|^2 (\mu_i + ||a_i||^2/\beta)}
\end{equation}
Since we are studying non-strongly convex problems, $\mu_i=0$. From Eq.~\ref{eq:bounding_duality_gap} we have
\begin{align}
    G(x_i^t) - G(x_i^{t+1}) &\leq G_i^t\left(\frac{\beta}{|\kappa_i^t|||a_i||^2}\right)
\end{align}


\section{Regret analysis: Commutative rewards?}
One problem with the analysis in Section~\ref{section:naive_bounding_path_length} is that we use \emph{dynamic regret}. This describes a setting where the adversary can decide a set of loss values at each time step, where loss values \emph{do not depend on prior actions}. However, in this setting, we are clearly dealing with a \emph{policy regret} problem, where reward values depend not only on the current action but the entire sequence of preceding actions. 

The paper~\cite{heidari2016tight_policy_regret} discusses (asymtotically) tight policy regret algorithms for bandits where the rewards decay or increase as a function of the number of pulls of the arm. In this setting, the reward function depends only on the \emph{number of times} the arm is pulled, not the order in which the arms are pulled. 
In particular, they discuss a setting where the reward obtained from pulling arm $k$ is given by a function $f_k(t_k)$, where $t_k$ is the number of times we have pulled arm $k$. They provide asymptotically tight bounds for the case where the $f_k$'s are monotonically increasing, bounded, and concave (and also monotonically decreasing and bounded).

The problem with this setting is that it is too restrictive for our problem: the reward obtained by choosing coordinate $k$ depends not only on the number of times we have chosen that coordinate, but also the number of times we have chosen all other coordinates. However, there are some nice ideas here, which we might be able to exploit.

In particular, let's assume that (for whatever reward function we decide upon), it satisfies a \emph{commutative property}. In particular, this means that the reward function is independent of the order in which we choose coordinates. (For certain update rules and objective functions, this might not be the case, but it might be approximately true.\footnote{\textbf{Research question}: under what scenarios (objective functions/update rules) are coordinate updates exactly commutative? Are coordinate descent updates ever exactly commmutative?}). Let's assume we have the reward function defined as follows:
\begin{equation}
    f(n_1, n_2, \ldots, n_d)
\end{equation}\
This is the reward obtained from pulling arm (coordinate) 1 $n_1$ times, 2 $n_2$ times, $d$ $n_d$ times, etc. Notice that this definition naturally satisfies our assumption of commutativity.

Depending on how we define our rewards, they might decrease or increase. For sake of argument, suppose our rewards are \emph{increasing} (which, e.g., might come from definining the reward as the sum of $r_i$'s). In particular, we have
\begin{equation}
    f(n_1, n_2, \ldots, n_i + 1, \ldots, n_d) \geq f(n_1, n_2, \ldots, n_i, \ldots, n_d)
\end{equation}
Let's also assume that the rewards are concave (i.e., we have diminishing returns on pulling an arm):
\begin{align}
    f(n_1, n_2, \ldots, n_i + 1, \ldots, n_d) -f(n_1, n_2, \ldots, n_i, \ldots, n_d) \leq \\
    f(n_1, n_2, \ldots, n_i, \ldots, n_d) -f(n_1, n_2, \ldots, n_i-1, \ldots, n_d)
\end{align}

Now we generalize the Algorithm 1 from~\cite{heidari2016tight_policy_regret}. This algorithm relies on an \emph{optimistic estimate} and a \emph{pessimistic estimate} for each arm. When the optimistic estimate for some arm $i$ is less than the pessimistic estimate for some other arm $j$, we can eliminate that arm from our consideration (since we are guaranteed to get a better cumulative reward by simply pulling arm $j$).

Under assumption of increasing rewards, the pessimistic estimate for arm $i$ is similar to before:
\begin{equation}
    q_i^t(T) = f(n_1, n_2, \ldots, n_i, \ldots, n_d) * (T - t)
\end{equation}
The optimistic estimate is a little bit harder. Using concavity, we know that 
\begin{align}
    f(n_1, \ldots, n_i + t, \ldots, n_d) \leq \\t \left[f(n_1,\ldots n_i,\ldots, n_d) - f(n_1,\ldots, n_i-1,\ldots, n_d)\right] + f(n_1,\ldots,n_i,\ldots,n_d)
\end{align}

However, we also want to include the possibility of playing some other arms, then playing arm $i$. So we can factor in the optimistic estimate from playing arm $j$ first, obtaining
\begin{align}
    f(n_1, \ldots, n_i + t, \ldots, n_j+s, \ldots, n_d) \leq \\t \left[f(n_1,\ldots n_i,\ldots, n_d) - f(n_1,\ldots, n_i-1,\ldots, n_d)\right]
     + f(n_1,\ldots,n_i,\ldots,n_d) \\
     + \text{optimistic estimate for arm $j$}
\end{align}

Hence, if we want to bound the total reward we can obtain from arm $i$ over a horizon $T$, we can obtain the following optimsitic estimate:
\begin{align}
    p_i^t(T) = \max_j p_j^{t-1}(T) + \sum_{k=1}^{T-t} k \left[f(n_1,\ldots n_i,\ldots, n_d) - f(n_1,\ldots, n_i-1,\ldots, n_d)\right] \\+ (T-t) f(n_1,\ldots, n_i,\ldots, n_d)
\end{align}

Substituting these new definitions for $p$ and $q$ leads to a bandit algorithm for this setting. \textbf{Unanswered questions:}
\begin{enumerate}
\item Does this have sublinear policy regret? (To get sublinear policy regret, we definitely also need to assume that $f(\cdot) \in [0,1]$, as in~\cite{heidari2016tight_policy_regret})
\item If this algorithm \emph{does not have} sublinear policy regret, it is still possible to get useful bounds if the policy regret is linear with a good coefficient (notice that Eq.~\ref{eq:suboptimality_with_regret} has a regret term in the denominator).
\item How could the algorithm be modified for better performance? The algorithm in~\cite{heidari2016tight_policy_regret} does not do any tricks to bias towards better arms that have not yet been eliminated; could some improvements be made?
\item Is the commutativity assumption valid? (From some preliminary computational experiments with $\ell_1$-regularized logistic regression (FISTA), permuting the order of updates does seem to lead to different loss values; however, random permutations seem to \emph{approximately} preserve the loss value--is \emph{approximate} commutativity enough?). 
\item Maybe this direction isn't valid? Perhaps it would be better to combine policy regret with dynamic regret?
\item Increasing reward functions for the rewards defined in Section~\ref{sec:new_definition_for_rewards}: Can we redefine rewards to be increasing in this setting? My guess is yes: we can get the bound

\begin{align}
    \epsilon(x^T) &\leq \frac{1}{\frac{1}{\epsilon(x^0)}  + \frac{\beta}{8TL^2} \sum_{t=1}^T\sum_{s=1}^t \frac{1}{\hat{\eta}_s^2}}
    % &\leq \frac{1}{\frac{1}{\epsilon(x^0)}  + \frac{\beta}{8L^2} \sum_{i=1}^t \frac{1}{\hat{\eta}_i^2} - \text{Reg}(V,T)}
\end{align}

where the new reward becomes the sum of the previous rewards, i.e., $\sum_{s=1}^t \frac{1}{\hat{\eta}_s^2}$. This new definition of rewards is monotonically increasing and concave. (Not sure if commutative, though).
\end{enumerate}

\subsection{Computational experiments}

I implemented some of the methods above, including:
\begin{itemize}
\item max\_r - select the coordinate with the largest $r_i$ according to the rewards in~\cite{salehi2018coordinate}
\item B\_max\_r - select the coordinate with the largest $r_i$ according the last observed reward. Every $E$ epochs, recompute all rewards. In addition, select a random arm with probability $\varepsilon$.
\item max\_eta - select the coordinate with the largest $\eta\equiv G_i/G$.
\item B\_max\_eta - use the same policy as B\_max\_r except use the reward $G_i$ rather than $r_i$.
\item B\_max\_eta - use the same policy as B\_max\_r except use the reward $G_i/G$ rather than $r_i$.
\end{itemize}

\begin{figure}[htp]
    \centering
    \includegraphics[width=\linewidth]{comparison_policies.pdf}
    \caption{A comparison of the different coordinate descent policies on a logistic regression problem, aloi.}
\end{figure}

% \subsection{Overview of ideas (9/24/19)}

% \cite{salehi2018coordinate} 

% Let
% \begin{equation}
%     R_i^t = \sum_{j = 1}^t r_i^j
% \end{equation}

% Let's assume we have a regret bound of the form
% \begin{equation}
%     \sum_{t=1}^T R_i^t - \sum_{t=1}^T R_{\istar}^t \geq -V \log T
% \end{equation}

% where $V$ is a bound on the path length of the comparator sequence (i.e., the best sequence). In this situation,

% \begin{equation}
%     V \leq \sum_{t=1}^T |R_{\istar}^t - R_{\istar}^t|.
% \end{equation}

% Note that we always have 
% \begin{equation}
%     R_{\istar}^t \geq R_i^t
% \end{equation}

% Now, note that $R_i^0 = R_{\istar}^0 = 0$. This, combined with the fact that $R_i^t \leq R_i^{t+1}$ and $R_{\istar}^t \leq R_{\istar}^{t+1}$, allows us to bound the progress term-wise:

% \begin{equation}
%     R_i^t - R_{\istar}^t \geq -V\left(\log t - \log(t-1)\right)
% \end{equation}

% which implies that

% \begin{equation}
%     \epsilon(x^0) - \epsilon(x^t) \geq R_i^t \geq R_{\istar}^t - \frac{2V}{t}
% \end{equation}

% Now we can use the fact that

% \begin{align}
%     r_{\istar}^t &\geq G_{\istar}(x^t)/2 \\
%     &\geq \frac{G(x_t)}{2d} \\
%     &\geq 
%     &\geq G_i(x_t)/\eta \comment{assumption in Prop. 2}
% \end{align}

% This, in turn, implies that

% \begin{equation}
%     \epsilon(x^t) \leq \frac{1}{\eta + 1}\left[\eta \epsilon(x^0) - \sum_{j=1}^t \epsilon(x^j) + \frac{2 V \eta}{t} \right]
% \end{equation}

% This is a recurrence relation that we can solve.

% In fact, we can strengthen this result if we use the fact that $R_{\istar}^t \geq R_i^t$ (this follows from the fact that $R_{\istar}^t = \sum_{j=1}^t \argmax_i r_i^j$). This leads to a recurrence that does not even involve $\eta$:

% \begin{equation}
%     R_i^t \leq \
% \end{equation}


\bibliographystyle{plain}
\bibliography{references}
\end{document}
